<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Naga Venkata Satyanarayana Devarapalli - AI/ML Professional Portfolio</title>
<style>
  /* RESET */
  *, *::before, *::after {
    box-sizing: border-box;
  }
  body {
    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
    background-color: #f8f5f2;
    color: #222;
    max-width: 1100px;
    margin: 0 auto;
    line-height: 1.6;
  }
  /* NAVIGATION */
  nav {
    background: #d35400;
    position: sticky;
    top: 0;
    z-index: 1000;
    box-shadow: 0 4px 8px rgba(0,0,0,0.15);
  }
  nav ul {
    list-style: none;
    margin: 0; padding: 1rem 2rem;
    display: flex;
    justify-content: center;
    gap: 2.5rem;
  }
  nav ul li a {
    text-decoration: none;
    color: white;
    font-weight: 700;
    font-size: 1.1rem;
    transition: color 0.3s ease;
  }
  nav ul li a:hover, nav ul li a:focus {
    color: #f39c12;
  }
  /* HEADER */
  header {
    text-align: center;
    padding: 3rem 1rem 1rem 1rem;
  }
  header h1 {
    font-size: 3.4rem;
    margin-bottom: 0;
    color: #d35400;
  }
  header p.subtitle {
    font-size: 1.3rem;
    font-weight: 600;
    margin-top: -0.3rem;
    color: #a84100;
  }
  /* SECTIONS */
  section {
    background: white;
    margin: 2rem 1rem;
    padding: 2rem 3rem;
    border-radius: 14px;
    box-shadow: 0 4px 12px rgba(211, 84, 0, 0.15);
  }
  section h2 {
    border-bottom: 3px solid #d35400;
    padding-bottom: 0.3rem;
    margin-bottom: 1.5rem;
    font-size: 2.4rem;
    color: #2c3e50;
  }
  section h3 {
    font-size: 1.7rem;
    margin-top: 2rem;
    margin-bottom: 1rem;
    color: #34495e;
  }
  section p {
    font-size: 1.1rem;
    margin-bottom: 1.4rem;
    color: #333;
  }
  section ul {
    margin-left: 1.3rem;
    margin-bottom: 1.5rem;
    font-size: 1.1rem;
  }
  section ul li {
    margin-bottom: 0.8rem;
  }
  /* ARTIFACT GRID */
  .artifact-grid {
    display: grid;
    grid-template-columns: repeat(auto-fit, minmax(320px, 1fr));
    gap: 2rem;
  }
  .artifact-card {
    background-color: #fff8f0;
    border-radius: 12px;
    padding: 1.5rem 2rem;
    box-shadow: 0 1px 7px rgba(211, 84, 0, 0.2);
    border: 1px solid #e1b87a;
  }
  .artifact-card h3 {
    margin-top: 0;
    color: #d35400;
  }
  .artifact-card p {
    color: #444;
  }
  .artifact-card a.btn {
    background-color: #d35400;
    color: white;
    padding: 0.7rem 1.3rem;
    border-radius: 9px;
    font-weight: 700;
    font-size: 1.05rem;
    text-align: center;
    display: inline-block;
    margin-top: 1rem;
    transition: background-color 0.3s ease;
  }
  .artifact-card a.btn:hover, .artifact-card a.btn:focus {
    background-color: #a84100;
  }
  /* CODE BLOCKS */
  pre, code {
    background: #f9f5f2;
    font-family: 'Courier New', Courier, monospace;
    border-radius: 10px;
    padding: 1rem 1.3rem;
    overflow-x: auto;
    font-size: 0.95rem;
    margin-bottom: 1.5rem;
  }
  /* CHAT TRANSCRIPTS */
  .chat-transcript {
    max-height: 350px;
    overflow-y: auto;
    padding: 1rem 1.2rem;
    background-color: #fff;
    border-radius: 10px;
    border: 1px solid #ddd;
    margin-bottom: 2rem;
  }
  .chat-msg-container {
    display: flex;
    flex-direction: column;
    gap: 1rem;
  }
  .chat-msg {
    padding: 0.7rem 1rem;
    border-radius: 15px;
    max-width: 70%;
  }
  .chat-user {
    background-color: #d35400;
    color: white;
    align-self: flex-end;
  }
  .chat-bot {
    background-color: #eee;
    color: #222;
    align-self: flex-start;
  }
  /* TABLE STYLING */
  table {
    border-collapse: collapse;
    width: 100%;
    margin-bottom: 2rem;
  }
  th, td {
    border: 1px solid #d35400;
    padding: 0.6rem 1rem;
    text-align: left;
    font-size: 1.1rem;
  }
  th {
    background-color: #f7c39b;
  }
  /* FOOTER */
  footer {
    text-align: center;
    padding: 2rem;
    background-color: #d35400;
    color: white;
    font-weight: 700;
    border-radius: 0 0 14px 14px;
    margin-top: 4rem;
  }
  /* RESPONSIVE */
  @media (max-width: 720px) {
    nav ul {
      flex-direction: column;
      gap: 1rem;
    }
    section {
      padding: 1rem 1.2rem;
      margin: 1rem 0.5rem;
    }
  }
</style>
</head>
<body>

<nav aria-label="Primary site navigation">
  <ul>
    <li><a href="#about-me">About Me</a></li>
    <li><a href="#artifacts">Artifacts Overview</a></li>
    <li><a href="#artifact4">Artifact 4</a></li>
    <li><a href="#artifact6">Artifact 6</a></li>
    <li><a href="#contact">Contact</a></li>
  </ul>
</nav>

<header role="banner">
  <h1>Naga Venkata Satyanarayana Devarapalli</h1>
  <p class="subtitle">Master of Science in Artificial Intelligence – Data Analytics</p>
</header>

<main>

<section id="about-me" aria-label="About Me section">
  <h2>About Me</h2>
  <p>
    Greetings! I am Naga Venkata Satyanarayana Devarapalli, a graduate student at Indiana Wesleyan University specializing in Artificial Intelligence and Data Analytics. My passion lies in leveraging data science and AI technologies to solve complex problems and foster innovation in business and society.
  </p>
  <p>
    With over a decade of experience spanning data analytics, business intelligence, and performance testing, I am extending my skills into advanced AI/ML techniques. This portfolio is a curated collection of artifacts highlighting my journey, practical projects, and research in the AI space.
  </p>
  <p>
    I am committed to ethical AI practices and transparent communication of data insights to empower informed decision-making.
  </p>
</section>

<section id="artifacts" aria-label="Portfolio Artifacts Overview">
  <h2>Portfolio Artifacts Overview</h2>
  <p>
    Below are summaries of key artifacts from my academic and professional work in AI and machine learning. Click on each to navigate to detailed views.
  </p>
  
  <div class="artifact-grid" role="list">
    <article class="artifact-card" role="listitem" id="artifact1-summary">
      <h3>Artifact 1: COVID-19 Vaccination Data Analysis</h3>
      <p>A data visualization and statistical analysis project examining COVID-19 vaccination trends and outcomes in India using RStudio.</p>
      <a href="#artifact1" class="btn" tabindex="0">View Artifact 1</a>
    </article>

    <article class="artifact-card" role="listitem" id="artifact2-summary">
      <h3>Artifact 2: AI & Machine Learning Evolution Timeline</h3>
      <p>An interactive timeline showcasing major milestones in AI and machine learning history, with emphasis on ethical and technological advances.</p>
      <a href="#artifact2" class="btn" tabindex="0">View Artifact 2</a>
    </article>

    <article class="artifact-card" role="listitem" id="artifact3-summary">
      <h3>Artifact 3: Big Data Adoption in Healthcare</h3>
      <p>Case study analysis of big data challenges in healthcare using the MIMIC-III dataset, focusing on data privacy and integration strategies.</p>
      <a href="#artifact3" class="btn" tabindex="0">View Artifact 3</a>
    </article>

    <article class="artifact-card" role="listitem" id="artifact4-summary">
      <h3>Artifact 4: Data Challenge Scenarios - AI Coach Interaction</h3>
      <p>Detailed interactions with an AI chatbot tackling common machine learning data issues including quality, bias, and privacy.</p>
      <a href="#artifact4" class="btn" tabindex="0">View Artifact 4</a>
    </article>

    <article class="artifact-card" role="listitem" id="artifact6-summary">
      <h3>Artifact 6: AI-Powered Predictive Analytics Project</h3>
      <p>Design and implementation of predictive sales models using Python and Power BI, showcasing end-to-end AI project skills.</p>
      <a href="#artifact6" class="btn" tabindex="0">View Artifact 6</a>
    </article>
  </div>
</section>

<!-- Artifact 4 Detailed Section -->
<section id="artifact4" aria-label="Artifact 4 Full Detail">
  <h2>Artifact 4: Data Challenge Scenarios - AI Coach Interaction</h2>
  
  <p>
    This artifact reflects my deep engagement with the Data Challenge Scenarios Coach chatbot on SchoolAI.com. I explored three major data challenges that machine learning practitioners face:
  </p>
  <ul>
    <li><strong>Data Quality and Cleaning:</strong> Addressing missing values, outliers, and inconsistent data.</li>
    <li><strong>Bias and Fairness:</strong> Mitigating demographic imbalances and monitoring fairness.</li>
    <li><strong>Data Privacy and Security:</strong> Ensuring compliance with privacy regulations and securing sensitive data.</li>
  </ul>

  <h3>Scenario 1: Data Quality Challenges</h3>
  <p><strong>Context:</strong> Preparing datasets with missing or noisy data for training machine learning models.</p>
  
  <div class="chat-transcript" aria-label="Chatbot dialogue for data quality">
    <div class="chat-msg-container">
      <div class="chat-bot chat-msg"><strong>Chatbot:</strong> How do you handle missing data and outliers in your dataset?</div>
      <div class="chat-user chat-msg">I analyze missingness patterns, apply imputation techniques, and identify outliers using statistical methods like IQR or Z-score.</div>
      <div class="chat-bot chat-msg">What imputation strategies do you prefer for numerical and categorical data?</div>
      <div class="chat-user chat-msg">Median or mean imputation for numerical data and mode or most frequent value for categorical data.</div>
      <div class="chat-bot chat-msg">How do you ensure consistent data formatting?</div>
      <div class="chat-user chat-msg">By standardizing date formats and categorical labels, and encoding variables appropriately.</div>
    </div>
  </div>

  <h4>Sample Python Code: Imputation of Missing Data</h4>
  <pre><code>import pandas as pd
from sklearn.impute import SimpleImputer

data = pd.read_csv('data.csv')
numeric_cols = data.select_dtypes(include=['float64', 'int64']).columns
categorical_cols = data.select_dtypes(include=['object']).columns

# Impute numeric columns with median
imputer_num = SimpleImputer(strategy='median')
data[numeric_cols] = imputer_num.fit_transform(data[numeric_cols])

# Impute categorical columns with mode
imputer_cat = SimpleImputer(strategy='most_frequent')
data[categorical_cols] = imputer_cat.fit_transform(data[categorical_cols])
</code></pre>

  <h3>Scenario 2: Bias and Fairness in AI</h3>
  <p><strong>Context:</strong> Detecting and mitigating bias in datasets to build fairer models.</p>

  <div class="chat-transcript" aria-label="Chatbot dialogue for bias and fairness">
    <div class="chat-msg-container">
      <div class="chat-bot chat-msg"><strong>Chatbot:</strong> Your dataset has an underrepresented group. What strategies do you use to mitigate bias?</div>
      <div class="chat-user chat-msg">I apply oversampling techniques like SMOTE, use fairness-aware models, and evaluate fairness metrics regularly.</div>
      <div class="chat-bot chat-msg">How do you measure fairness?</div>
      <div class="chat-user chat-msg">By metrics like demographic parity, equalized odds, and disparate impact ratio.</div>
    </div>
  </div>

  <h4>Fairness Metrics Summary</h4>
  <table>
    <thead>
      <tr><th>Metric</th><th>Description</th><th>Application</th></tr>
    </thead>
    <tbody>
      <tr><td>Demographic Parity</td><td>Equal positive prediction rates across groups</td><td>Ensures equal opportunity</td></tr>
      <tr><td>Equalized Odds</td><td>Equal true positive and false positive rates</td><td>Balances error rates</td></tr>
      <tr><td>Disparate Impact</td><td>Ratio of favorable outcomes between groups</td><td>Detects potential discrimination</td></tr>
    </tbody>
  </table>

  <h4>Sample Python Code: Oversampling Using SMOTE</h4>
  <pre><code>from imblearn.over_sampling import SMOTE
import pandas as pd

data = pd.read_csv('data.csv')
X = data.drop('target', axis=1)
y = data['target']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)
</code></pre>

  <h3>Scenario 3: Data Privacy and Security</h3>
  <p><strong>Context:</strong> Protecting sensitive data while enabling machine learning development and collaboration.</p>

  <div class="chat-transcript" aria-label="Chatbot dialogue for privacy and security">
    <div class="chat-msg-container">
      <div class="chat-bot chat-msg"><strong>Chatbot:</strong> How do you protect sensitive health data to comply with privacy regulations?</div>
      <div class="chat-user chat-msg">By pseudonymizing identifiers, encrypting data storage, controlling access, and applying differential privacy techniques.</div>
      <div class="chat-bot chat-msg">How can data be shared securely?</div>
      <div class="chat-user chat-msg">Using federated learning, secure multi-party computation, or sharing aggregated data only.</div>
    </div>
  </div>

  <h4>Sample Python Code: Pseudonymization</h4>
  <pre><code>import hashlib
import pandas as pd

def pseudonymize(value):
    return hashlib.sha256(value.encode('utf-8')).hexdigest()

data = pd.read_csv('sensitive_data.csv')
data['patient_id_hash'] = data['patient_id'].apply(pseudonymize)
data.drop(columns=['patient_id'], inplace=True)
</code></pre>

  <h3>Reflection and Learning Outcomes</h3>
  <p>
    This AI coach interaction deepened my understanding of practical data challenges in ML workflows. Real-time dialogue and feedback facilitated active learning, critical thinking, and application of best practices in data cleaning, bias mitigation, and privacy protection.
  </p>
</section>

<!-- Artifact 6 Full Section -->
<section id="artifact6" aria-label="Artifact 6 Full Detail">
  <h2>Artifact 6: AI-Powered Predictive Analytics Project</h2>
  
  <p>
    This artifact highlights my experience designing, building, and deploying predictive models to forecast retail sales trends. Combining Python programming for data science and Power BI for visualization, this project demonstrates full lifecycle AI capabilities.
  </p>

  <h3>Project Overview</h3>
  <p>
    The goal was to create a reliable sales forecasting model using historical sales data enriched with seasonal, promotional, and economic features. This project required extensive data cleaning, feature engineering, model training, and dashboard development.
  </p>

  <h3>Data Collection and Cleaning</h3>
  <p>
    Data was sourced from transactional sales records spanning five years. Missing values, outliers, and inconsistent formats were cleaned using pandas and sklearn tools.
  </p>

  <h4>Sample Python Code: Handling Missing Values and Outliers</h4>
  <pre><code>import pandas as pd
import numpy as np

data = pd.read_csv('sales_data.csv')

# Fill missing numerical values with median
data['sales'].fillna(data['sales'].median(), inplace=True)

# Remove outliers using IQR method
Q1 = data['sales'].quantile(0.25)
Q3 = data['sales'].quantile(0.75)
IQR = Q3 - Q1
filtered_data = data[(data['sales'] >= (Q1 - 1.5 * IQR)) & (data['sales'] <= (Q3 + 1.5 * IQR))]
</code></pre>

  <h3>Feature Engineering</h3>
  <p>
    Created lag features, rolling averages, and encoded categorical variables to enhance model inputs.
  </p>

  <h4>Sample Python Code: Creating Lag Features and Rolling Averages</h4>
  <pre><code>data['sales_lag_1'] = data['sales'].shift(1)
data['sales_rolling_mean_3'] = data['sales'].rolling(window=3).mean()
data['is_promotion'] = data['promotion'].apply(lambda x: 1 if x == 'Yes' else 0)
</code></pre>

  <h3>Modeling</h3>
  <p>
    Used scikit-learn's RandomForestRegressor to train the predictive model. Evaluated performance using RMSE and R² metrics.
  </p>

  <h4>Sample Python Code: Model Training and Evaluation</h4>
  <pre><code>from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

features = ['sales_lag_1', 'sales_rolling_mean_3', 'is_promotion', 'holiday_flag']
X = data[features].dropna()
y = data.loc[X.index, 'sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

predictions = model.predict(X_test)

rmse = mean_squared_error(y_test, predictions, squared=False)
r2 = r2_score(y_test, predictions)

print(f'RMSE: {rmse}')
print(f'R² Score: {r2}')
</code></pre>

  <h3>Power BI Dashboard</h3>
  <p>
    Developed interactive dashboards in Power BI to visualize sales trends, forecast accuracy, and feature importance, enabling stakeholders to monitor business metrics intuitively.
  </p>

  <p><em>Note: Due to format constraints, dashboard visuals are linked externally or available upon request.</em></p>

  <h3>Project Challenges and Solutions</h3>
  <ul>
    <li><strong>Challenge:</strong> Handling seasonality and promotions impact on sales variability.</li>
    <li><strong>Solution:</strong> Incorporated time-series features and promotional flags in modeling.</li>
    <li><strong>Challenge:</strong> Missing and noisy data points.</li>
    <li><strong>Solution:</strong> Data cleaning and imputation methods described above.</li>
  </ul>

  <h3>Reflection</h3>
  <p>
    This project strengthened my practical skills in data science workflows, from raw data to actionable insights. Combining Python analytics and Power BI visualization bridged technical and business perspectives effectively.
  </p>
</section>

<section id="contact" aria-label="Contact Information">
  <h2>Contact Me</h2>
  <p>Email: <a href="mailto:tone_satya@yahoo.co.in">tone_satya@yahoo.co.in</a></p>
  <p>Location: Dallas, TX, United States</p>
  <p>LinkedIn: <a href="https://www.linkedin.com/in/naga-venkata-satyanarayana-devarapalli" target="_blank" rel="noopener noreferrer">linkedin.com/in/naga-venkata-satyanarayana-devarapalli</a></p>
</section>

</main>

<footer role="contentinfo">
  <p>© 2025 Naga Venkata Satyanarayana Devarapalli. All Rights Reserved.</p>
</footer>

</body>
</html>
